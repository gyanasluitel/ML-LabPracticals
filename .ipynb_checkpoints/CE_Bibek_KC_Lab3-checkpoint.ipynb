{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\",\"Hard disk format problems\"]\n",
    "X =vectorizer.fit_transform(content)\n",
    "print(X.toarray().transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n",
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'this', 'toy']\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "DIR = r\"C:\\Users\\bibek\\Desktop\\Course Materials\\CE 401\\Machine Learning (Elective)\\BuildingMachineLearningSystemsWithPython-master\\ch03\\data\\toy\"\n",
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples,num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples,num_features))\n",
    "print(vectorizer.get_feature_names())\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "new_post =\"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1,v2):\n",
    "    delta =v1-v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist=2.00: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist=1.41: Imaging databases store data.\n",
      "=== Post 4 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "Best post is 3 with dist=1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "for i in range(0,num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    print(\"=== Post %i with dist=%.2f: %s\"%(i, d, post))\n",
    "    if d<best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "print(\"Best post is %i with dist=%.2f\"%(best_i, best_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')\n",
    "sorted(vectorizer.get_stop_words())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk.stem\n",
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")\n",
    "s.stem(\"imaging\")\n",
    "s.stem(\"image\")\n",
    "s.stem(\"imagination\")\n",
    "s.stem(\"imagine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"buys\")\n",
    "s.stem(\"buying\")\n",
    "s.stem(\"bought\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedCountVectorizer(min_df=1,stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def tfidf(term, doc, corpus):\n",
    "    tf = doc.count(term) / len(doc)\n",
    "    num_docs_with_term = len([d for d in corpus if term in d])\n",
    "    idf = sp.log(len(corpus) / num_docs_with_term)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]\n",
    "print(tfidf(\"a\", a, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27031007207210955\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13515503603605478\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3662040962227032\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import *\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (\n",
    "            s.stem(w) for w in analyzer(doc))\n",
    "vectorizer = StemmedTfidfVectorizer(min_df=1,stop_words='english', decode_error='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "all_data = sklearn.datasets.fetch_20newsgroups(subset='all')\n",
    "print(len(all_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(all_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sklearn.datasets.fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532\n"
     ]
    }
   ],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test')\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3529\n"
     ]
    }
   ],
   "source": [
    "groups = ['comp.graphics', 'comp.os.ms-windows.misc','comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware','comp.windows.x', 'sci.space']\n",
    "train_data = sklearn.datasets.fetch_20newsgroups(subset='train',categories=groups)\n",
    "print(len(train_data.filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349\n"
     ]
    }
   ],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(subset='test',\n",
    "categories=groups)\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 3529, #features: 4712\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=10, max_df=0.5,stop_words='english', decode_error='ignore')\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples,\n",
    "num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration  0, inertia 5899.560\n",
      "Iteration  1, inertia 3218.298\n",
      "Iteration  2, inertia 3184.333\n",
      "Iteration  3, inertia 3164.867\n",
      "Iteration  4, inertia 3152.004\n",
      "Iteration  5, inertia 3143.111\n",
      "Iteration  6, inertia 3136.256\n",
      "Iteration  7, inertia 3129.325\n",
      "Iteration  8, inertia 3124.567\n",
      "Iteration  9, inertia 3121.900\n",
      "Iteration 10, inertia 3120.210\n",
      "Iteration 11, inertia 3118.627\n",
      "Iteration 12, inertia 3117.363\n",
      "Iteration 13, inertia 3116.811\n",
      "Iteration 14, inertia 3116.588\n",
      "Iteration 15, inertia 3116.417\n",
      "Iteration 16, inertia 3115.760\n",
      "Iteration 17, inertia 3115.374\n",
      "Iteration 18, inertia 3115.155\n",
      "Iteration 19, inertia 3114.949\n",
      "Iteration 20, inertia 3114.515\n",
      "Iteration 21, inertia 3113.937\n",
      "Iteration 22, inertia 3113.720\n",
      "Iteration 23, inertia 3113.548\n",
      "Iteration 24, inertia 3113.475\n",
      "Iteration 25, inertia 3113.447\n",
      "Converged at iteration 25: center shift 0.000000e+00 within tolerance 2.069005e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=300,\n",
       "    n_clusters=50, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=3, tol=0.0001, verbose=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 50\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=num_clusters, init='random', n_init=1,verbose=1, random_state=3)\n",
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 17 47 ... 41 14 16]\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = \"Disk drive problems. Hi, I have a problem with my hard disk.After 1 year it is working only sporadically now.I tried to format it, but now it doesn't boot any more.Any ideas? Thanks.\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_indices = (km.labels_==new_post_label).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "similar = []\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm((new_post_vec - vectorized[i]).toarray())\n",
    "    similar.append((dist, train_data.data[i]))\n",
    "similar = sorted(similar)\n",
    "print(len(similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[int(len(similar)/10)]\n",
    "show_at_3 = similar[int(len(similar)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(245, 'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization: The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk\\n\\n==============================================================================\\n', 'comp.graphics')\n"
     ]
    }
   ],
   "source": [
    "post_group = zip(train_data.data, train_data.target)\n",
    "all = [(len(post[0]), post[0], train_data.target_names[post[1]]) for post in post_group]\n",
    "graphics = sorted([post for post in all if post[2]=='comp.graphics'])\n",
    "print(graphics[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['situnaya', 'ibm3090', 'bham', 'ac', 'uk', 'subject', 'test', 'sorri', 'organ', 'univers', 'birmingham', 'unit', 'kingdom', 'line', 'nntp', 'post', 'host', 'ibm3090', 'bham', 'ac', 'uk']\n"
     ]
    }
   ],
   "source": [
    "noise_post = graphics[5][1]\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "print(list(analyzer(noise_post)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test', 'uk', 'unit', 'univers']\n"
     ]
    }
   ],
   "source": [
    "useful = set(analyzer(noise_post)).intersection(vectorizer.get_feature_names())\n",
    "print(sorted(useful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF(ac)=3.51\n",
      "IDF(birmingham)=6.77\n",
      "IDF(host)=1.74\n",
      "IDF(kingdom)=6.68\n",
      "IDF(nntp)=1.77\n",
      "IDF(sorri)=4.14\n",
      "IDF(test)=3.83\n",
      "IDF(uk)=3.70\n",
      "IDF(unit)=4.42\n",
      "IDF(univers)=1.91\n"
     ]
    }
   ],
   "source": [
    "for term in sorted(useful):\n",
    "    print('IDF(%s)=%.2f'%(term,vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
