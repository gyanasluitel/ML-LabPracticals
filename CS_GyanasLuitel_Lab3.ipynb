{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "still-projection",
   "metadata": {},
   "source": [
    "# Chapter 3: Clustering - Finding related posts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-wichita",
   "metadata": {},
   "source": [
    "- Name: Gyanas Luitel\n",
    "- Group: Computer Science\n",
    "- Roll No: 27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-management",
   "metadata": {},
   "source": [
    "## Measuring the relatedness of posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "encouraging-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-theta",
   "metadata": {},
   "source": [
    "## Preprocessing - similarlity measured as a similar number of common words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-wheat",
   "metadata": {},
   "source": [
    "### Converting raw text into a bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "celtic-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "precious-number",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer()\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "italian-annex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to format my hard disk', 'Hard disk format problems']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = [\"How to format my hard disk\", \"Hard disk format problems\"]\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smoking-marker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disk', 'format', 'hard', 'how', 'my', 'problems', 'to']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "previous-worker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(x.toarray().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-graph",
   "metadata": {},
   "source": [
    "**Counting words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "southern-hayes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01.txt', '02.txt', '03.txt', '04.txt', '05.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = \"./Chapter03/data/toy\"\n",
    "sorted(os.listdir(DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "likely-tower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is a toy post about machine learning. Actually, it contains not much interesting stuff.',\n",
       " 'Imaging databases provide storage capabilities.',\n",
       " 'Most imaging databases save images permanently.\\n',\n",
       " 'Imaging databases store data.',\n",
       " 'Imaging databases store data. Imaging databases store data. Imaging databases store data.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = [open(os.path.join(DIR, f)).read() for f in os.listdir(DIR)]\n",
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "oriented-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lovely-anaheim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "irish-denial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'actually', 'capabilities', 'contains', 'data', 'databases', 'images', 'imaging', 'interesting', 'is', 'it', 'learning', 'machine', 'most', 'much', 'not', 'permanently', 'post', 'provide', 'save', 'storage', 'store', 'stuff', 'this', 'toy']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "transsexual-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post = \"imaging databases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "upper-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unnecessary-company",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "assigned-lyric",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-reality",
   "metadata": {},
   "source": [
    "Similarlilty measurement (the naive one) using Euclidean distance between the count vectors of the new post and all the old posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "royal-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "logical-trustee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 2.00: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 1.41: Imaging databases store data.\n",
      "=== Post 4 with dist = 5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 3 with dist = 1.41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    if post == new_post:\n",
    "        continue\n",
    "        \n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "    \n",
    "    print(f\"=== Post {i} with dist = {d:.2f}: {post}\")\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i \n",
    "\n",
    "print()\n",
    "print(f\"Best post is {best_i} with dist = {best_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "measured-vampire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "[[0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.getrow(3).toarray())\n",
    "print(X_train.getrow(4).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-earthquake",
   "metadata": {},
   "source": [
    "**Normalizing word count vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "precious-velvet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta = v1_normalized - v2_normalized \n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "discrete-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.92: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 0.77: Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 3 with dist = 0.77\n"
     ]
    }
   ],
   "source": [
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    if post == new_post:\n",
    "        continue\n",
    "        \n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    \n",
    "    print(f\"=== Post {i} with dist = {d:.2f}: {post}\")\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i \n",
    "\n",
    "print()\n",
    "print(f\"Best post is {best_i} with dist = {best_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collectible-encounter",
   "metadata": {},
   "source": [
    "**Removing less important words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "severe-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df = 1, stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "thermal-soccer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(vectorizer.get_stop_words())[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dimensional-blackjack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 5, #features: 18\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(f\"samples: {num_samples}, #features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "variable-investment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n"
     ]
    }
   ],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "noble-dylan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.86: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 0.77: Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 3 with dist = 0.77\n"
     ]
    }
   ],
   "source": [
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    if post == new_post:\n",
    "        continue\n",
    "        \n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    \n",
    "    print(f\"=== Post {i} with dist = {d:.2f}: {post}\")\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i \n",
    "\n",
    "print()\n",
    "print(f\"Best post is {best_i} with dist = {best_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-israeli",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "olympic-background",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sought-california",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphic'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "italic-greek",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imaging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "interim-catering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imag'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "legitimate-current",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"imagination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "compatible-pacific",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imagin'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('imagine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "informal-kidney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"buys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "comparable-might",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'buy'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"buying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "homeless-arena",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bought'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"bought\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-affiliation",
   "metadata": {},
   "source": [
    "**Extending the vectorizer with NLTK's stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "metallic-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer = nltk.stem.SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "isolated-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedCountVectorizer(min_df = 1, stop_words = \"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bottom-sound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: 5, #features: 17\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(f\"samples: {num_samples}, #features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "empirical-vacuum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n"
     ]
    }
   ],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "turned-university",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.63: Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 0.77: Imaging databases store data.\n",
      "=== Post 4 with dist = 0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 2 with dist = 0.63\n"
     ]
    }
   ],
   "source": [
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    if post == new_post:\n",
    "        continue\n",
    "        \n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    \n",
    "    print(f\"=== Post {i} with dist = {d:.2f}: {post}\")\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i \n",
    "\n",
    "print()\n",
    "print(f\"Best post is {best_i} with dist = {best_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-sheet",
   "metadata": {},
   "source": [
    "**Stop words on steroids**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "tracked-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(term, doc, corpus):\n",
    "    tf = doc.count(term) / len(doc)\n",
    "    num_docs_with_term = len([d for d in corpus if term in d])\n",
    "    idf = np.log(len(corpus) / num_docs_with_term)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "accurate-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, abb, abc = [\"a\"], [\"a\", \"b\", \"b\"], [\"a\", \"b\", \"c\"]\n",
    "D = [a, abb, abc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "imposed-reservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", a, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "collaborative-witch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "adapted-vatican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "advisory-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27031007207210955\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abb, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "progressive-authentication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"a\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "distributed-oregon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13515503603605478\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"b\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "white-parking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3662040962227032\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(\"c\", abc, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "palestinian-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedTfidfVectorizer(min_df = 1, stop_words = \"english\", decode_error = \"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "romance-update",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "num_samples, num_features = X_train.shape\n",
    "print(f\"#samples: {num_samples}, #features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "changed-mystery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t0.7071067811865476\n",
      "  (0, 4)\t0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "new_post = \"imaging databases\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "monthly-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist = 1.41 : This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist = 1.08 : Imaging databases provide storage capabilities.\n",
      "=== Post 2 with dist = 0.86 : Most imaging databases save images permanently.\n",
      "\n",
      "=== Post 3 with dist = 0.92 : Imaging databases store data.\n",
      "=== Post 4 with dist = 0.92 : Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "\n",
      "Best post is 2 with dist = 0.86\n"
     ]
    }
   ],
   "source": [
    "best_doc = None\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i, post in enumerate(posts):\n",
    "    if post == new_post:\n",
    "        continue\n",
    "        \n",
    "    post_vec = X_train.getrow(i)\n",
    "    \n",
    "    d = dist_norm(post_vec, new_post_vec)\n",
    "    \n",
    "    print(f\"=== Post {i} with dist = {d:.2f} : {post}\")\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i \n",
    "        \n",
    "print()\n",
    "print(f\"Best post is {best_i} with dist = {best_dist:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-classroom",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-period",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-ghana",
   "metadata": {},
   "source": [
    "### Getting test data to evaluate our ideas on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "quick-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "otherwise-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n"
     ]
    }
   ],
   "source": [
    "all_data = sklearn.datasets.fetch_20newsgroups(subset = \"all\")\n",
    "print(len(all_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "revolutionary-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(all_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "stone-clinton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11314\n"
     ]
    }
   ],
   "source": [
    "train_data = sklearn.datasets.fetch_20newsgroups(subset = 'train')\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "emerging-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7532\n"
     ]
    }
   ],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(subset = 'test')\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "convertible-reducing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3529\n"
     ]
    }
   ],
   "source": [
    "groups = ['comp.graphics' , 'comp.os.ms-windows.misc', \n",
    "          'comp.sys.ibm.pc.hardware','comp.sys.mac.hardware',\n",
    "          'comp.windows.x', 'sci.space'\n",
    "         ]\n",
    "train_data = sklearn.datasets.fetch_20newsgroups(subset=\"train\", categories = groups)\n",
    "print(len(train_data.filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "liable-snowboard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2349\n"
     ]
    }
   ],
   "source": [
    "test_data = sklearn.datasets.fetch_20newsgroups(subset = \"test\", categories = groups)\n",
    "print(len(test_data.filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-consistency",
   "metadata": {},
   "source": [
    "### Clustering Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "explicit-scenario",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 3529, #features: 4712\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df = 10, \n",
    "                                    max_df = 0.5, stop_words = 'english',\n",
    "                                   decode_error = 'ignore')\n",
    "vectorized = vectorizer.fit_transform(train_data.data)\n",
    "num_samples, num_features = vectorized.shape\n",
    "print('#samples: %d, #features: %d' % (num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "abroad-speaker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization complete\n",
      "Iteration 0, inertia 5899.5595831471655\n",
      "Iteration 1, inertia 3218.297747726279\n",
      "Iteration 2, inertia 3184.3328334733214\n",
      "Iteration 3, inertia 3164.867358130041\n",
      "Iteration 4, inertia 3152.003949571175\n",
      "Iteration 5, inertia 3143.1109963529184\n",
      "Iteration 6, inertia 3136.2559774422048\n",
      "Iteration 7, inertia 3129.3248717684405\n",
      "Iteration 8, inertia 3124.56747982014\n",
      "Iteration 9, inertia 3121.9001105797406\n",
      "Iteration 10, inertia 3120.209894571872\n",
      "Iteration 11, inertia 3118.62745619288\n",
      "Iteration 12, inertia 3117.362525978361\n",
      "Iteration 13, inertia 3116.8112664390364\n",
      "Iteration 14, inertia 3116.587892365764\n",
      "Iteration 15, inertia 3116.417048753848\n",
      "Iteration 16, inertia 3115.760414808626\n",
      "Iteration 17, inertia 3115.3736535034473\n",
      "Iteration 18, inertia 3115.155454436256\n",
      "Iteration 19, inertia 3114.9491175607545\n",
      "Iteration 20, inertia 3114.5149932662175\n",
      "Iteration 21, inertia 3113.9369169464094\n",
      "Iteration 22, inertia 3113.719999300366\n",
      "Iteration 23, inertia 3113.547519005385\n",
      "Iteration 24, inertia 3113.474905847476\n",
      "Iteration 25, inertia 3113.4467573371267\n",
      "Converged at iteration 25: strict convergence.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(init='random', n_clusters=50, n_init=1, random_state=3, verbose=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 50\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = num_clusters, init = 'random', n_init = 1, verbose = 1, random_state = 3)\n",
    "km.fit(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "homeless-argentina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38 17 47 ... 41 14 16]\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "unlimited-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3529,)\n"
     ]
    }
   ],
   "source": [
    "print(km.labels_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-ridge",
   "metadata": {},
   "source": [
    "## Solving our initial challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "wooden-manner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Disk drive problems. Hi, I have a problem with my hard disk. After 1 year it is working only sporadically now.I tried to format it, but now it doesn't boot any more.Any ideas? Thanks.\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post = \"Disk drive problems. Hi, I have a problem with my hard \\\n",
    "disk. After 1 year it is working only sporadically now.\\\n",
    "I tried to format it, but now it doesn't boot any more.\\\n",
    "Any ideas? Thanks.\"\n",
    "new_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "paperback-completion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_post_vec = vectorizer.transform([new_post])\n",
    "new_post_label = km.predict(new_post_vec)[0]\n",
    "new_post_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "incoming-germany",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_indices = (km.labels_ == new_post_label).nonzero()[0]\n",
    "# similar_indices\n",
    "len(similar_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "mounted-providence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "similar = []\n",
    "\n",
    "for i in similar_indices:\n",
    "    dist = sp.linalg.norm(new_post_vec - vectorized[i].toarray())\n",
    "    similar.append((dist, train_data.data[i]))\n",
    "similar = sorted(similar)\n",
    "print(len(similar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "brown-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_at_1 = similar[0]\n",
    "show_at_2 = similar[int(len(similar)/10)]\n",
    "show_at_3 = similar[int(len(similar)/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "interracial-manufacturer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0378441731334074 \t From: Thomas Dachsel <GERTHD@mvs.sas.com>\n",
      "Subject: BOOT PROBLEM with IDE controller\n",
      "Nntp-Posting-Host: sdcmvs.mvs.sas.com\n",
      "Organization: SAS Institute Inc.\n",
      "Lines: 25\n",
      "\n",
      "Hi,\n",
      "I've got a Multi I/O card (IDE controller + serial/parallel\n",
      "interface) and two floppy drives (5 1/4, 3 1/2) and a\n",
      "Quantum ProDrive 80AT connected to it.\n",
      "I was able to format the hard disk, but I could not boot from\n",
      "it. I can boot from drive A: (which disk drive does not matter)\n",
      "but if I remove the disk from drive A and press the reset switch,\n",
      "the LED of drive A: continues to glow, and the hard disk is\n",
      "not accessed at all.\n",
      "I guess this must be a problem of either the Multi I/o card\n",
      "or floppy disk drive settings (jumper configuration?)\n",
      "Does someone have any hint what could be the reason for it.\n",
      "Please reply by email to GERTHD@MVS.SAS.COM\n",
      "Thanks,\n",
      "Thomas\n",
      "+-------------------------------------------------------------------+\n",
      "| Thomas Dachsel                                                    |\n",
      "| Internet: GERTHD@MVS.SAS.COM                                      |\n",
      "| Fidonet:  Thomas_Dachsel@camel.fido.de (2:247/40)                 |\n",
      "| Subnet:   dachsel@rnivh.rni.sub.org (UUCP in Germany, now active) |\n",
      "| Phone:    +49 6221 4150 (work), +49 6203 12274 (home)             |\n",
      "| Fax:      +49 6221 415101                                         |\n",
      "| Snail:    SAS Institute GmbH, P.O.Box 105307, D-W-6900 Heidelberg |\n",
      "| Tagline:  One bad sector can ruin a whole day...                  |\n",
      "+-------------------------------------------------------------------+\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "1.1716497460538475 \t From: badry@cs.UAlberta.CA (Badry Jason Theodore)\n",
      "Subject: Chaining IDE drives\n",
      "Summary: Trouble with Master/Slave drives\n",
      "Nntp-Posting-Host: cab009.cs.ualberta.ca\n",
      "Organization: University Of Alberta, Edmonton Canada\n",
      "Lines: 16\n",
      "\n",
      "Hi.  I am trying to set up a Conner 3184 and a Quantum 80AT drive.  I have\n",
      "the conner set to the master, and the quantum set to the slave (doesn't work\n",
      "the other way around).  I am able to access both drives if I boot from a \n",
      "floppy, but the drives will not boot themselves.  I am running MSDOS 6, and\n",
      "have the Conner partitioned as Primary Dos, and is formatted with system\n",
      "files.  I have tried all different types of setups, and even changed IDE\n",
      "controller cards.  If I boot from a floppy, everything works great (except\n",
      "the booting part :)).  The system doesn't report an error message or anything,\n",
      "just hangs there.  Does anyone have any suggestions, or has somebody else\n",
      "run into a similar problem?  I was thinking that I might have to update the bios\n",
      "on one of the drives (is this possible?).  Any suggestions/answers would be\n",
      "greatly appreciated.  Please reply to:\n",
      "\n",
      "\tJason Badry\n",
      "\tbadry@cs.ualberta.ca\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "1.3118266609870635 \t From: delman@mipg.upenn.edu (Delman Lee)\n",
      "Subject: Tandberg 3600 + Future Domain TMC-1660 + Seagate ST-21M problem??\n",
      "Distribution: comp\n",
      "Organization: University of Pennsylvania, USA.\n",
      "Lines: 37\n",
      "Nntp-Posting-Host: mipgsun.mipg.upenn.edu\n",
      "\n",
      "I am trying to get my system to work with a Tandberg 3600 + Future\n",
      "Domain TMC-1660 + Seagate ST-21M MFM controller. \n",
      "\n",
      "The system boots up if the Tandberg is disconnected from the system,\n",
      "and of course no SCSI devices found (I have no other SCSI devices).\n",
      "\n",
      "The system boots up if the Seagate MFM controller is removed from the\n",
      "system. The Future Domain card reports finding the Tandberg 3660 on\n",
      "the SCSI bus. The system then of course stops booting because my MFM\n",
      "hard disks can't be found.\n",
      "\n",
      "The system hangs if all three (Tandberg, Future Domain TMC-1660 &\n",
      "Seagate MFM controller) are in the system. \n",
      "\n",
      "Looks like there is some conflict between the Seagate and Future\n",
      "Domain card. But the funny thing is that it only hangs if the Tandberg\n",
      "is connected.\n",
      "\n",
      "I have checked that there are no conflict in BIOS addresses, IRQ & I/O\n",
      "port. Have I missed anything?\n",
      "\n",
      "I am lost here. Any suggestions are most welcomed. Thanks in advance.\n",
      "\n",
      "Delman.\n",
      "\n",
      "\n",
      "\n",
      "--\n",
      "______________________________________________________________________\n",
      "\n",
      "  Delman Lee                                 Tel.: +1-215-662-6780\n",
      "  Medical Image Processing Group,            Fax.: +1-215-898-9145\n",
      "  University of Pennsylvania,\n",
      "  4/F Blockley Hall, 418 Service Drive,                         \n",
      "  Philadelphia, PA 19104-6021,\n",
      "  U.S.A..                            Internet: delman@mipg.upenn.edu\n",
      "______________________________________________________________________\n",
      "\n",
      "--------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in [show_at_1, show_at_2, show_at_3]:\n",
    "    print(f\"{i[0]} \\t {i[1]}\")\n",
    "    print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-anderson",
   "metadata": {},
   "source": [
    "### Another Look at Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "blind-rugby",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(245,\n",
       " 'From: SITUNAYA@IBM3090.BHAM.AC.UK\\nSubject: test....(sorry)\\nOrganization: The University of Birmingham, United Kingdom\\nLines: 1\\nNNTP-Posting-Host: ibm3090.bham.ac.uk\\n\\n==============================================================================\\n',\n",
       " 'comp.graphics')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_group = zip(train_data.data, train_data.target)\n",
    "all = [(len(post[0]), post[0], train_data.target_names[post[1]]) for post in post_group]\n",
    "graphics = sorted([post for post in all if post[2] == 'comp.graphics'])\n",
    "graphics[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "reasonable-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['situnaya', 'ibm3090', 'bham', 'ac', 'uk', 'subject', 'test', 'sorri', 'organ', 'univers', 'birmingham', 'unit', 'kingdom', 'line', 'nntp', 'post', 'host', 'ibm3090', 'bham', 'ac', 'uk']\n"
     ]
    }
   ],
   "source": [
    "noise_post = graphics[5][1]\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "print(list(analyzer(noise_post)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "respiratory-farming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ac', 'birmingham', 'host', 'kingdom', 'nntp', 'sorri', 'test', 'uk', 'unit', 'univers']\n"
     ]
    }
   ],
   "source": [
    "useful = set(analyzer(noise_post)).intersection(vectorizer.get_feature_names())\n",
    "print(sorted(useful))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "resident-superior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF(ac) = 3.51    \n",
      "IDF(birmingham) = 6.77    \n",
      "IDF(host) = 1.74    \n",
      "IDF(kingdom) = 6.68    \n",
      "IDF(nntp) = 1.77    \n",
      "IDF(sorri) = 4.14    \n",
      "IDF(test) = 3.83    \n",
      "IDF(uk) = 3.70    \n",
      "IDF(unit) = 4.42    \n",
      "IDF(univers) = 1.91    \n"
     ]
    }
   ],
   "source": [
    "for term in sorted(useful):\n",
    "    print(f\"IDF({term}) =\\\n",
    " {vectorizer._tfidf.idf_[vectorizer.vocabulary_[term]]:.2f}\\\n",
    "    \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-patio",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
